### K近邻法 
一种基本分类和回归方法。k近邻法利用训练数据集对特征向量空间进行划分，并作为其分类的 **“模型”** 。**k值的选择、距离度量及分类决策规则**为三要素 

#### k近邻算法 
> 输入：$T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}， x_i\in \mathcal X\subseteq R^n,y_i\in \mathcal Y=\{c_1,c_2,\dots, c_k\}$ 
> 
> 输出：实例$x$所属的类y 
> 
> 1. 根据指定的**距离度量**，在$T$中查找$x$的**最近邻的$k$个点**，覆盖这$k$个点的$x$的邻域定义为$N_k(x)$
>  1. 在$N_k(x)$中应用**分类决策规则**决定$x$的类别$y$
    $$
    y=\arg\max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j), i=1,2,\dots,N, j=1,2,\dots,K
    $$

#### 距离度量 
**特征空间**中的两个实例点的距离是两个实例点相似程度的反映 
$$L_p(x_i, x_j)=\left(\sum_{l=1}^{n}{\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^p}\right)^{\frac{1}{p}}$$ 

#### k值的选择 
- k值的选择直接影响算法结果，较大、较小都容易出现问题 
- 通过交叉验证选取最优k值 

#### 分类决策规则 
往往是多数表决规则（majority voting rule，等价于经验风险最小化）：如果分类的损失为0-1损失函数，那么**误分类率**为 
 $\frac{1}{k}\sum_{x_i\in N_k(x)}{I(y_i\ne c_i)}=1-\frac{1}{k}\sum_{x_i\in N_k(x)}{I(y_i= c_i)}$ 


#### KD树
