## 隐马尔可夫模型 
隐马尔可夫模型（hidden Markov model,HMM）是关于时序的概率模型，描述由一个**隐藏**的马尔可夫链随机生成**不可观测的状态序列**，再由各个状态序列产生观测序列的过程。 

### 隐含马尔可夫模型

$$
P(s_1,s_2,s_3,\dots,o_1,o_2,o_3,\dots)=\prod_tP(s_t|s_{t-1})\cdot P(o_t|s_t)
$$

隐含的是**状态**$s$

隐含马尔可夫模型由**初始概率分布**(向量$\pi$), **状态转移概率分布**(矩阵$A$)以及**观测概率分布**(矩阵$B$)确定.

隐马尔可夫模型$\lambda$ 可以用三元符号表示, 即
$$
\lambda = (A, B, \pi)
$$

其中$A,B,\pi$称为模型三要素。

#### 两个假设 
1. 齐次马尔可夫假设（状态）  
$$
P(i_t|i_{t-1},o_{t-1},\dots,i_1,o_1) = P(i_t|i_{t-1}), t=1,2,\dots,T
$$ 

1. 观测独立性假设（观测） 
 $$
   P(o_t|i_T,o_T,i_{T-1},o_{T-1},\dots,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},\dots,i_1,o_1)=P(o_t|i_t)
   $$ 

#### 三个基本问题 
1. 概率计算问题
   输入: 模型$\lambda=(A,B,\pi)$, 观测序列$O=(o_1,o_2,\dots,o_T)$
   输出: $P(O|\lambda)$

1. 学习问题
   输入: 观测序列 $O=(o_1,o_2,\dots,o_T)$
   输出: **极大似然估计**模型$\lambda=(A,B,\pi)$

1. 预测问题, 也称为解码问题(Decoding)
   输入: 模型$\lambda=(A,B,\pi)$, 观测序列$O=(o_1,o_2,\dots,o_T)$ 
   输出: 给定观测序列**条件概率$P(I|O)$最大**的状态序列 $I=(i_1,i_2,\dots,i_T)$

   因为状态序列是隐藏的，不可观测的，所以叫解码。


### 概率计算模型 

#### 前向算法 
给定马尔可夫模型$\lambda$, 定义到时刻$t$部分观测序列为$o_1, o_2, \dots ,o_t$, 且状态$q_i$的概率为**前向概率**, 记作
$$\alpha_t(i)=P(o_1,o_2,\dots,o_t,i_t=q_i|\lambda)$$  

**观测序列概率的前向算法**如下：
1. 初值 ：
$$
 \alpha_1(i)=\pi_ib_i(o_1), i=1,2,\dots,N  
$$

1. 递推,对$t=1,2,\dots,T-1$:
$$
   \alpha_{t+1}(i) = \left[\sum\limits_{j=1}^N\alpha_t(j)a_{ji}\right]b_i(o_{t+1})\color{black}, \   i=1,2,\dots,N, \ t = 1,2,\dots,T-1 
$$

1. 终止 
$$
   P(O|\lambda)=\sum\limits_{i=1}^N\alpha_T(i)
$$ 

实际上是基于**状态序列的路径结构**的递推计算$P(P|\lambda)$的算法。每个 $\alpha_{t+1}(i)$的计算利用前$N个\alpha_t(j)$ 


#### 后向算法 
给定马尔可夫模型$\lambda$, 定义到时刻$t$状态为$q_i$的条件下, 从$t+1$到$T$的部分观测序列为$o_{t+1}, o_{t+2}, \dots ,o_T$的概率为**后向概率**, 记作
$$
\beta_t(i)=P(o_{t+1},o_{t+2},\dots,o_T|i_t=q_i, \lambda)
$$  

**观测序列概率的后向算法**如下

1. 终值
 $$
 \beta_T(i)=1, i=1,2,\dots,N
 $$ 在$t=T$时刻, 观测序列已经确定. 

1. 递推，对于$t=T-1,T-2,\dots,1$ 
$$
\beta_t(i)=\sum\limits_{j=1}^Na_{ij}b_j(o_{t+1})\beta_{t+1}(j)\color{black}, i=1,2,\dots,N, t=T-1, T-2,\dots,1
$$  

1. 
$$
P(O|\lambda)=\sum\limits_{i=1}^N\pi_ib_i(o_1)\beta_1(i) 
$$  

其实前向和后向不是为了求整个序列$O$的概率，是为了求中间的某个点$t$，前向后向主要是有这个关系:
$$
\alpha_t(i)\beta_t(i)=P(i_t=q_i,O|\lambda)
$$
当$t=1$或者$t=T-1$的时候，单独用后向和前向就可以求得$P(O|\lambda)$，分别利用前向和后向算法均可以求解$P(O|\lambda)$，结果一致。
故有：
$$
P(O|\lambda)=\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1}\beta_{t+1}(j)),\ t=1,2,\dots,T-1
$$



#### 概率和期望  
1. 给定模型$\lambda$和观测$O$在时刻$t$处于状态$q_i$的概率。记 ：
$$
\begin{aligned}
 \gamma_t(i) = P(i_t=q_i| O,\lambda) \\ 
 = \frac {P(i_t=q_i,O|\lambda)} {P(O|\lambda) } \\ 
 = \frac {\alpha_t(i)\beta_t(i)} {\sum_{j=1}^N\alpha_t(j)\beta_t(j)}      
\end{aligned}
$$ 

1. 给定模型$\lambda$和观测$O$在时刻$t$处于状态$q_i$且时刻$t+1$处于状态$q_j$的概率。记 
$$
\begin{aligned}
\xi_t(i,j) = P(i_t=q_i,i_{t+1}=q_j|O,\lambda) \\
= \frac {\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)} {\sum\limits_{i=1}^N  \sum\limits_{j=1}^N \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j) }
\end{aligned}
$$ 

1. 在观测$O$下状态$i$出现的期望值 
$$
\sum\limits_{t=1}^T \gamma_t(i)
$$
1. 在观测$O$下状态$i$转移的期望值 
$$
\sum\limits_{t=1}^{T-1} \gamma_t(i)
$$
1. 在观测$O$下状态$i$转移到状态$j$的期望值
$$
\sum\limits_{t=1}^{T-1} \xi_t(i,j) 
$$


### 学习算法

#### 无监督学习算法-Baum-Welch（EM）算法  

马尔可夫模型实际上是一个含有隐变量的概率模型
$$
P(O|\lambda)=\sum\limits_IP(O|I,\lambda)P(I|\lambda)
$$ 

> 输入: 观测数据$O=(o_1, o_2, \dots, o_T)$
>
> 输出: 隐马尔可夫模型参数
>
> 1. 初始化
> 对$n=0$，选取$a_{ij}^{(0)}, b_j(k)^{(0)}, \pi_i^{(0)}$，得到模型参数$\lambda^{(0)}=(A^{(0)}, B^{(0)},\pi^{(0)})$
>
> 1. 递推
> 对$n=1,2,\dots,$
> $$
> a_{ij}^{(n+1)}=\frac{\sum\limits_{t=1}^{T-1}\xi_t(i,j)}{\sum\limits_{t=1}^{T-1}\gamma_t(i)}
> $$
>
> $$
> b_j(k)^{(n+1)}=\frac{\sum\limits_{t=1,o_t=v_k}^{T}\gamma_t(j)}{\sum\limits_{t=1}^T\gamma_t(j)}
> $$
>
> $$
> \pi_i^{(n+1)}=\gamma_1(i)
> $$
>
> 1. 终止
> 得到模型参数$\lambda^{(n+1)}=(A^{(n+1)}, B^{(n+1)},\pi^{(n+1)})$


其中EM算法的E步求Q函数： 
$$
Q(\lambda,\bar{\lambda})=\sum_I\log P(O,I|\lambda)P(O,I|\bar\lambda)
$$ M步：极大化$Q$函数$Q(\lambda,\bar{\lambda})$求模型参数$\pi,A,B$ 

### 预测算法

// TODO 
#### 维特比算法(Viterbi)

> 输入: 模型$\lambda=(A, B, \pi)$和观测$O=(o_1, o_2,\dots,o_T)$
>
> 输出: 最优路径$I^*=(i_1^*, i_2^*,\dots,i_T^*)$
>
> 1. 初始化
>    $\delta_1(i)=\pi_ib_i(o_1), i=1,2,\dots,N$
>    $\psi_1(i)=0, i=1,2,\dots,N$
> 1. 递推
>    $t=2,3,\dots,T$
>    $\delta_t(i)=\max\limits_{1\leq j \leq N}\left[\delta_{t-1}(j)a_{ji}\right]b_i(o_t), i=1,2,\dots,N$
>    $\psi_t(j)=\arg\max\limits_{1\leq j \leq N}\left[\delta_{t-1}(j)a_{ji}\right], i=1,2,\dots,N$
> 1. 终止
>    $P^*=\max\limits_{1\leq i\leq N}\delta_T(i)$
>    $i_T^*=\arg\max\limits_{1\leq i \leq N}\left[ \delta_T(i)\right]$
> 1. 最优路径回溯
>    $t=T-1, T-2, \dots,1$
>    $i_t^*=\psi_{t+1}(i_{i+1}^*)$
