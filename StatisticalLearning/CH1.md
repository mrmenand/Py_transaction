## 统计学习 
统计学习方法三要素：模型，策略，算法 
> 1. 得到一个有限的训练数据集合 
> 2. 确定包含所有可能的模型的**假设空间**，即学习模型的集合 
> 3. 确定模型选择的准则，即学习的**策略** 
> 4. 实现求解最优模型的算法，即学习的**算法** 
> 5. 通过学习方法选择最优的模型 
> 6. 利用学习的最优模型对新数据进行预测或分析 

QAQ : 与深度学习中的Model（模型）、Loss(学习准则)、Optimizer(优化器)的区别和差异 



### 监督学习 
从给定有限的训练数据出发，**假设**数据是独立同分布的，而且假设模型属于某个假设空间，应用某一评价准则，从假设空间中选取一个最优的模型，使它对已给的训练数据以及未知测试数据在给定评价标准意义下有最准确的预测。
> 1. 输入空间、特征空间和输出空间 
> 2. 联合概率分布 
> 3. 假设空间（hypothesis space）：条件概率分布P(Y|X)和决策函数（decision function）Y =f（X）
> 4. 问题形式化

#### 模型 
- 所要学习的条件概率分布或决策函数,模型的假设空间包含所有的条件概率分布或者决策函数   
- 决策函数 ：$F=\left(f|Y=f_\theta(x),\theta \in R^n \right)$
- 条件概率分布 ：$F=\left( P|P_\theta(Y|X),\theta \in R^n \right)$ 

#### 策略（选取最优模型）

损失函数与风险函数 
> **损失函数**度量模型**一次预测**的好坏，**风险函数**度量**平均意义**下模型预测的好坏

1. 损失函数(loss function)或代价函数(cost function)损失函数定义为给定输入$X$的**预测值$f(X)$**和**真实值$Y$**之间的**非负实值**函数，记作$L(Y,f(X))$ 

2. 风险函数(risk function)或期望损失(expected loss)这个和模型的泛化误差的形式是一样的
   $R_{exp}(f)=E_p[L(Y,f(X))]=\int{\mathcal X\times\mathcal Y}L(y,f(x))P(x,y)\, {\rm d}x{\rm d}y$
   模型$f(X)$关于联合分布$P(X,Y)$的**平均意义下的**损失(**期望**损失)，因为$P(X,Y)$是未知的 
   - **经验风险**(empirical risk)或**经验损失**(empirical loss)
   $R_{emp}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))$ 
   - **结构风险**(structural risk)
   $R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$
   $J(f)$为模型复杂度, $\lambda \geqslant 0$是系数，用以权衡经验风险和模型复杂度。

### 算法（求解最优模型） 
统计学习问题转化成最优化问题（如何找到**全局最优解**）
 - 有显式解析解, 对应的最优化问题比较简单 
 - 通常解析解不存在, 需要通过数值计算的方式求解 

### 模型评估与模型选择 
- 训练误差和测试误差
- 过拟合：所选模型的复杂度比真模型的复杂度更高（在测试数据集表现的很差） 
- 模型选择  
 - 正则化：选择经验风险与模型复杂度同时较小的模型（奥卡姆剃刀：很到解释已知数据并且十分简单才是最好的模型）
 - 交叉验证 
   - 简单交叉验证
   - S折交叉验证 
   - 留一交叉验证 


### 泛化能力 
- 泛化误差（所学习的模型期望风险）：
$R_{exp}(\hat{f})=E_p[L(Y,f(X))]=\int_{\mathcal X\times\mathcal Y}L(y,f(x))P(x,y)\,{\rm d}x{\rm d}y$
- 泛化误差上界：样本容量的函数 


### 生成模型和判别模型 
- 生成模型：数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型
  $P（Y|X）=\frac{P(X,Y)}{P(X)} $
- 判别模型：直接学习**条件概率**$P(Y|X)$或者**决策函数**$f(X)$


TODO:极大似然估计和贝叶斯估计的区别以及增加相关模型推导和图表
